{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83aa6f82",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d3f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import PIL\n",
    "import gradio as gr\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e3850",
   "metadata": {},
   "source": [
    "### Import and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a079f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:4\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fedb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OWL-ViT model\n",
    "owlvit_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
    "owlvit_model.eval()\n",
    "owlvit_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "# run segment anything (SAM)\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=\"./sam_vit_h_4b8939.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df631602",
   "metadata": {},
   "source": [
    "### Custom function for mask and box creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1180e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  \n",
    "\n",
    "def plot_boxes_to_image(image_pil, tgt):\n",
    "    H, W = tgt[\"size\"]\n",
    "    boxes = tgt[\"boxes\"]\n",
    "    labels = tgt[\"labels\"]\n",
    "    assert len(boxes) == len(labels), \"boxes and labels must have same length\"\n",
    "\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    mask = Image.new(\"L\", image_pil.size, 0)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # draw boxes and masks\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # random color\n",
    "        color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "        # draw\n",
    "        x0, y0, x1, y1 = box\n",
    "        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "        draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        if hasattr(font, \"getbbox\"):\n",
    "            bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "        else:\n",
    "            w, h = draw.textsize(str(label), font)\n",
    "            bbox = (x0, y0, w + x0, y0 + h)\n",
    "        # bbox = draw.textbbox((x0, y0), str(label))\n",
    "        draw.rectangle(bbox, fill=color)\n",
    "        draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "        mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "\n",
    "    return image_pil, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f16e8c",
   "metadata": {},
   "source": [
    "### Function logic for segmenting and detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7074f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_image(img, text_prompt):\n",
    "    # load image\n",
    "    if not isinstance(img, PIL.Image.Image):\n",
    "        pil_img = Image.fromarray(np.uint8(img)).convert('RGB')\n",
    "\n",
    "    text_prompt = text_prompt\n",
    "    texts = [text_prompt.split(\",\")]\n",
    "\n",
    "    box_threshold = 0.0\n",
    "\n",
    "    # run object detection model\n",
    "    with torch.no_grad():\n",
    "        inputs = owlvit_processor(text=texts, images=pil_img, return_tensors=\"pt\").to(device)\n",
    "        outputs = owlvit_model(**inputs)\n",
    "    \n",
    "    # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "    target_sizes = torch.Tensor([pil_img.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to COCO API\n",
    "    results = owlvit_processor.post_process_object_detection(outputs=outputs, threshold=box_threshold, target_sizes=target_sizes.to(device))\n",
    "    scores = torch.sigmoid(outputs.logits)\n",
    "    topk_scores, topk_idxs = torch.topk(scores, k=1, dim=1)\n",
    "    \n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "     \n",
    "    topk_idxs = topk_idxs.squeeze(1).tolist()\n",
    "    topk_boxes = results[i]['boxes'][topk_idxs]\n",
    "    topk_scores = topk_scores.view(len(text), -1)\n",
    "    topk_labels = results[i][\"labels\"][topk_idxs]\n",
    "    boxes, scores, labels = topk_boxes, topk_scores, topk_labels\n",
    "    \n",
    "    # boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "    \n",
    "\n",
    "    boxes = boxes.cpu().detach().numpy()\n",
    "    normalized_boxes = copy.deepcopy(boxes)\n",
    "    \n",
    "    # # visualize pred\n",
    "    size = pil_img.size\n",
    "    pred_dict = {\n",
    "        \"boxes\": normalized_boxes,\n",
    "        \"size\": [size[1], size[0]], # H, W\n",
    "        \"labels\": [text[idx] for idx in labels]\n",
    "    }\n",
    "\n",
    "    # release the OWL-ViT\n",
    "    # owlvit_model.cpu()\n",
    "    # del owlvit_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # run segment anything (SAM)\n",
    "    open_cv_image = np.array(pil_img) \n",
    "    image = cv2.cvtColor(open_cv_image, cv2.COLOR_BGR2RGB)\n",
    "    sam_predictor.set_image(image)\n",
    "    \n",
    "    H, W = size[1], size[0]\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        boxes[i] = torch.Tensor(boxes[i])\n",
    "\n",
    "    boxes = torch.tensor(boxes, device=sam_predictor.device)\n",
    "\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes, image.shape[:2])\n",
    "    \n",
    "    masks, _, _ = sam_predictor.predict_torch(\n",
    "        point_coords = None,\n",
    "        point_labels = None,\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output = False,\n",
    "    )\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        show_mask(mask.cpu().numpy(), plt.gca(), random_color=True)\n",
    "    for box in boxes:\n",
    "        show_box(box.numpy(), plt.gca())\n",
    "    plt.axis('off')\n",
    "    \n",
    "    import io\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    owlvit_segment_image = Image.open(buf)\n",
    "\n",
    "    # grounded results\n",
    "    image_with_box = plot_boxes_to_image(pil_img, pred_dict)[0]\n",
    "    \n",
    "    return owlvit_segment_image, image_with_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178cc6c6",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f13003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    query_image, \n",
    "    inputs=[gr.Image(), \"text\"], \n",
    "    outputs=[\"image\", \"image\"],\n",
    "    title=\"Semantic Image Segmentation with Segment Anything (SAM) and OWL-ViT\"\n",
    ")\n",
    "\n",
    "demo.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4d91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
